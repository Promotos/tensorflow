{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset eli5 (C:/Users/mlach/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n"
     ]
    }
   ],
   "source": [
    "eli5 = load_dataset(\"eli5\", split=\"train_asks[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = eli5.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '11dbtl',\n",
       " 'title': 'Is it possible to add enough salt (or other solute) to water such that it is still an aqueous solution, but dense enough for a man to walk on?',\n",
       " 'selftext': \"Pretty much the title. If you added too much solute, you'd be walking on a wet, mucky solid I would think. Is it possible to add so much solute that the aqueous solution is still liquid but dense enough to support a grown man (say 150 pounds over a surface area of  1ft^2)\",\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers': {'a_id': ['c6lgnu1', 'c6ll34v'],\n",
       "  'text': ['Corn starch, but only if you run.  A supersaturated solution will become solid under sudden stress, but remain liquid otherwise.',\n",
       "   \"It would be supporting you by buoyancy, so the amount of liquid you displace would have to be equal to your weight to keep you up. Normally 150 pounds of water takes up 68 liters of space. In order for the water to stay below your ankles, you'd only be using your feet which take up maybe 2 liters at most (It would also suck for walking because there would be no friction and very little to balance yourself with). \\n\\nThis would require a liquid 34 times as dense as water. The densest liquid at standard temperature and pressure is Mercury at only about 13.6 six times the density of water. So the answer is basically no, your legs would be partially submerged and you'd have nothing to get traction on to move forward. edit: and severe mercury poisoning.\"],\n",
       "  'score': [6, 3]},\n",
       " 'title_urls': {'url': []},\n",
       " 'selftext_urls': {'url': []},\n",
       " 'answers_urls': {'url': []}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model_checkpoint = \"distilgpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '11dbtl',\n",
       " 'title': 'Is it possible to add enough salt (or other solute) to water such that it is still an aqueous solution, but dense enough for a man to walk on?',\n",
       " 'selftext': \"Pretty much the title. If you added too much solute, you'd be walking on a wet, mucky solid I would think. Is it possible to add so much solute that the aqueous solution is still liquid but dense enough to support a grown man (say 150 pounds over a surface area of  1ft^2)\",\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers.a_id': ['c6lgnu1', 'c6ll34v'],\n",
       " 'answers.text': ['Corn starch, but only if you run.  A supersaturated solution will become solid under sudden stress, but remain liquid otherwise.',\n",
       "  \"It would be supporting you by buoyancy, so the amount of liquid you displace would have to be equal to your weight to keep you up. Normally 150 pounds of water takes up 68 liters of space. In order for the water to stay below your ankles, you'd only be using your feet which take up maybe 2 liters at most (It would also suck for walking because there would be no friction and very little to balance yourself with). \\n\\nThis would require a liquid 34 times as dense as water. The densest liquid at standard temperature and pressure is Mercury at only about 13.6 six times the density of water. So the answer is basically no, your legs would be partially submerged and you'd have nothing to get traction on to move forward. edit: and severe mercury poisoning.\"],\n",
       " 'answers.score': [6, 3],\n",
       " 'title_urls.url': [],\n",
       " 'selftext_urls.url': [],\n",
       " 'answers_urls.url': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5 = eli5.flatten()\n",
    "\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80ac28bf48f41ec84d0661ace5fa8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c64c65ba805429984c53dbfb08565c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240e180de2334ac9a75dc01d0e647c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe44ac43b87b4dd7a9d79d38b12fd8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer, AdamWeightDecay\n",
    "\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM\n",
    "\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mf:\\git\\tensorflow\\huggingface\\Casual Language Modeling\\Hello World\\train.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/git/tensorflow/huggingface/Casual%20Language%20Modeling/Hello%20World/train.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer\u001b[39m.\u001b[39madd_special_tokens({\u001b[39m'\u001b[39m\u001b[39mpad_token\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/git/tensorflow/huggingface/Casual%20Language%20Modeling/Hello%20World/train.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tf_train_set \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mprepare_tf_dataset(\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/git/tensorflow/huggingface/Casual%20Language%20Modeling/Hello%20World/train.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     lm_dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/git/tensorflow/huggingface/Casual%20Language%20Modeling/Hello%20World/train.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/git/tensorflow/huggingface/Casual%20Language%20Modeling/Hello%20World/train.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/git/tensorflow/huggingface/Casual%20Language%20Modeling/Hello%20World/train.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     collate_fn\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/git/tensorflow/huggingface/Casual%20Language%20Modeling/Hello%20World/train.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/git/tensorflow/huggingface/Casual%20Language%20Modeling/Hello%20World/train.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tf_test_set \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mprepare_tf_dataset(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/git/tensorflow/huggingface/Casual%20Language%20Modeling/Hello%20World/train.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     lm_dataset[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/git/tensorflow/huggingface/Casual%20Language%20Modeling/Hello%20World/train.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/git/tensorflow/huggingface/Casual%20Language%20Modeling/Hello%20World/train.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/git/tensorflow/huggingface/Casual%20Language%20Modeling/Hello%20World/train.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     collate_fn\u001b[39m=\u001b[39mdata_collator,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/git/tensorflow/huggingface/Casual%20Language%20Modeling/Hello%20World/train.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1384\u001b[0m, in \u001b[0;36mTFPreTrainedModel.prepare_tf_dataset\u001b[1;34m(self, dataset, batch_size, shuffle, tokenizer, collate_fn, collate_fn_args, drop_remainder, prefetch)\u001b[0m\n\u001b[0;32m   1382\u001b[0m \u001b[39mif\u001b[39;00m drop_remainder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1383\u001b[0m     drop_remainder \u001b[39m=\u001b[39m shuffle\n\u001b[1;32m-> 1384\u001b[0m tf_dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mto_tf_dataset(\n\u001b[0;32m   1385\u001b[0m     columns\u001b[39m=\u001b[39;49mfeature_cols,\n\u001b[0;32m   1386\u001b[0m     label_cols\u001b[39m=\u001b[39;49mlabel_cols,\n\u001b[0;32m   1387\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1388\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1389\u001b[0m     drop_remainder\u001b[39m=\u001b[39;49mdrop_remainder,\n\u001b[0;32m   1390\u001b[0m     collate_fn\u001b[39m=\u001b[39;49mcollate_fn,\n\u001b[0;32m   1391\u001b[0m     collate_fn_args\u001b[39m=\u001b[39;49mcollate_fn_args,\n\u001b[0;32m   1392\u001b[0m     prefetch\u001b[39m=\u001b[39;49mprefetch,\n\u001b[0;32m   1393\u001b[0m )\n\u001b[0;32m   1394\u001b[0m \u001b[39mreturn\u001b[39;00m tf_dataset\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:411\u001b[0m, in \u001b[0;36mTensorflowDatasetMixin.to_tf_dataset\u001b[1;34m(self, batch_size, columns, shuffle, collate_fn, drop_remainder, collate_fn_args, label_cols, prefetch, num_workers)\u001b[0m\n\u001b[0;32m    407\u001b[0m     dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m    409\u001b[0m \u001b[39m# TODO(Matt, QL): deprecate the retention of label_ids and label\u001b[39;00m\n\u001b[1;32m--> 411\u001b[0m output_signature, columns_to_np_types \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49m_get_output_signature(\n\u001b[0;32m    412\u001b[0m     dataset,\n\u001b[0;32m    413\u001b[0m     collate_fn\u001b[39m=\u001b[39;49mcollate_fn,\n\u001b[0;32m    414\u001b[0m     collate_fn_args\u001b[39m=\u001b[39;49mcollate_fn_args,\n\u001b[0;32m    415\u001b[0m     cols_to_retain\u001b[39m=\u001b[39;49mcols_to_retain,\n\u001b[0;32m    416\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size \u001b[39mif\u001b[39;49;00m drop_remainder \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    417\u001b[0m )\n\u001b[0;32m    419\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output_signature:\n\u001b[0;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mlabel_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m columns \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m columns) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m columns:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:292\u001b[0m, in \u001b[0;36mTensorflowDatasetMixin._get_output_signature\u001b[1;34m(dataset, collate_fn, collate_fn_args, cols_to_retain, batch_size, num_test_batches)\u001b[0m\n\u001b[0;32m    290\u001b[0m static_shape \u001b[39m=\u001b[39m []\n\u001b[0;32m    291\u001b[0m \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(shapes[\u001b[39m0\u001b[39m])):\n\u001b[1;32m--> 292\u001b[0m     sizes \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([shape[dim] \u001b[39mfor\u001b[39;00m shape \u001b[39min\u001b[39;00m shapes])\n\u001b[0;32m    293\u001b[0m     \u001b[39mif\u001b[39;00m dim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    294\u001b[0m         static_shape\u001b[39m.\u001b[39mappend(batch_size)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:292\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    290\u001b[0m static_shape \u001b[39m=\u001b[39m []\n\u001b[0;32m    291\u001b[0m \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(shapes[\u001b[39m0\u001b[39m])):\n\u001b[1;32m--> 292\u001b[0m     sizes \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([shape[dim] \u001b[39mfor\u001b[39;00m shape \u001b[39min\u001b[39;00m shapes])\n\u001b[0;32m    293\u001b[0m     \u001b[39mif\u001b[39;00m dim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    294\u001b[0m         static_shape\u001b[39m.\u001b[39mappend(batch_size)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    lm_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    lm_dataset[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
