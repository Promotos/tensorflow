{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/mlearning-ai/building-your-own-mini-chatgpt-b2e9716ab119\n",
    "https://www.kaggle.com/code/changyeop/how-to-fine-tune-gpt-2-for-beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data into a pandas dataframe\n",
    "data = pd.read_csv(\"your_text_data.file\")\n",
    "\n",
    "# Preprocessing the data\n",
    "data = data.dropna()  # remove missing values\n",
    "data = data.drop_duplicates()  # remove duplicate values\n",
    "data = data.sample(frac=1)  # shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Instantiate a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Tokenize the text\n",
    "text = data['text'].values\n",
    "tokenized_text = tokenizer(text, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead\n",
    "\n",
    "# Instantiate a model\n",
    "model = AutoModelWithLMHead.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 1000,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# Instantiate a trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_text,\n",
    "    eval_dataset=tokenized_text\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "\n",
    "# Evaluate the model\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "# Extract the perplexity score\n",
    "perplexity = eval_result['perplexity']\n",
    "\n",
    "# Extract the BLEU score\n",
    "bleu_score = eval_result['bleu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Instantiate a text generation pipeline\n",
    "text_generator = pipeline(\"text-generation\", model=model)\n",
    "\n",
    "# Generate text\n",
    "generated_text = text_generator(\"The brown fine fox jumps over the lazy frog.\", max_length=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
